{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-10-31T20:44:13.511243Z",
     "iopub.execute_input": "2023-10-31T20:44:13.511671Z",
     "iopub.status.idle": "2023-10-31T20:44:29.956241Z",
     "shell.execute_reply.started": "2023-10-31T20:44:13.511637Z",
     "shell.execute_reply": "2023-10-31T20:44:29.954775Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "file_path = \"/kaggle/input/filtered/filtered.tsv\"\n",
    "raw_df = pd.read_csv(file_path, delimiter=\"\\t\")\n",
    "df = pd.DataFrame(raw_df)\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-31T21:12:11.627004Z",
     "iopub.execute_input": "2023-10-31T21:12:11.627703Z",
     "iopub.status.idle": "2023-10-31T21:12:13.602830Z",
     "shell.execute_reply.started": "2023-10-31T21:12:11.627670Z",
     "shell.execute_reply": "2023-10-31T21:12:13.602021Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Necessary inputs\n",
    "TOKEN_PREFIX = \"Make this text non-toxic:\"\n",
    "MAX_INPUT_LENGTH = 128\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "def prepare_model_inputs(examples):\n",
    "    input_texts = [TOKEN_PREFIX + ref for ref in examples[\"reference\"]]\n",
    "    target_texts = [tsn for tsn in examples[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(input_texts, max_length=MAX_INPUT_LENGTH, truncation=True, return_overflowing_tokens=False)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(target_texts, max_length=MAX_TARGET_LENGTH, truncation=True, return_overflowing_tokens=False)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"/kaggle/input/filtered/filtered.tsv\", sep='\\t', index_col=0)\n",
    "dataset = Dataset.from_pandas(df).remove_columns('__index_level_0__')\n",
    "\n",
    "# Split dataset\n",
    "split_dict = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Crop dataset\n",
    "batch_size = 256\n",
    "cropped_datasets = split_dict\n",
    "cropped_datasets['train'] = split_dict['train'].select(range(1000))\n",
    "cropped_datasets['test'] = split_dict['test'].select(range(100))\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = cropped_datasets.map(prepare_model_inputs, batched=True, batch_size=batch_size, remove_columns=split_dict[\"train\"].column_names)\n",
    "tokenized_datasets['train'][0]\n",
    "\n",
    "# Create model\n",
    "model_name = 't5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-detoxification\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    report_to='tensorboard',\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Postprocessing function\n",
    "def post_process_predictions(predictions, labels):\n",
    "    predictions = [pred.strip() for pred in predictions]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return predictions, labels\n",
    "\n",
    "# Metrics function\n",
    "def compute_custom_metrics(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = post_process_predictions(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_custom_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-31T22:22:50.040857Z",
     "iopub.execute_input": "2023-10-31T22:22:50.041249Z",
     "iopub.status.idle": "2023-10-31T22:24:45.871916Z",
     "shell.execute_reply.started": "2023-10-31T22:22:50.041212Z",
     "shell.execute_reply": "2023-10-31T22:24:45.870689Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3c2db966b2a45e3a95891e5e5b5382d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2450cc79b2c64c25883e3c0ed13ecca7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 01:49, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.810852</td>\n      <td>9.666900</td>\n      <td>10.800000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.554081</td>\n      <td>2.752200</td>\n      <td>6.400000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.410892</td>\n      <td>2.470900</td>\n      <td>6.400000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>1.375681</td>\n      <td>0.238900</td>\n      <td>4.800000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>1.387443</td>\n      <td>9.764300</td>\n      <td>9.200000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>1.344263</td>\n      <td>41.316000</td>\n      <td>10.600000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>1.348004</td>\n      <td>41.316000</td>\n      <td>10.600000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>1.318513</td>\n      <td>41.316000</td>\n      <td>10.600000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>1.302500</td>\n      <td>41.316000</td>\n      <td>10.600000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>1.301290</td>\n      <td>41.316000</td>\n      <td>10.600000</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "TrainOutput(global_step=250, training_loss=1.7819320068359374, metrics={'train_runtime': 110.242, 'train_samples_per_second': 4.535, 'train_steps_per_second': 2.268, 'total_flos': 16537012715520.0, 'train_loss': 1.7819320068359374, 'epoch': 10.0})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ... (Previous code remains the same)\n",
    "\n",
    "# Test the model\n",
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test Results:\", test_results)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_detox_model')\n",
    "tokenizer.save_pretrained('fine_tuned_detox_model')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-31T22:26:03.234850Z",
     "iopub.execute_input": "2023-10-31T22:26:03.235540Z",
     "iopub.status.idle": "2023-10-31T22:26:05.433444Z",
     "shell.execute_reply.started": "2023-10-31T22:26:03.235507Z",
     "shell.execute_reply": "2023-10-31T22:26:05.432488Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:00]\n    </div>\n    "
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test Results: {'eval_loss': 1.3012897968292236, 'eval_bleu': 41.316, 'eval_gen_len': 10.6, 'eval_runtime': 1.008, 'eval_samples_per_second': 4.96, 'eval_steps_per_second': 2.976, 'epoch': 10.0}\n",
     "output_type": "stream"
    },
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "('fine_tuned_detox_model/tokenizer_config.json',\n 'fine_tuned_detox_model/special_tokens_map.json',\n 'fine_tuned_detox_model/spiece.model',\n 'fine_tuned_detox_model/added_tokens.json',\n 'fine_tuned_detox_model/tokenizer.json')"
     },
     "metadata": {}
    }
   ]
  }
 ]
}

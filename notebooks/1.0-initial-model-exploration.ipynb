{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:17:28.547804Z",
     "iopub.execute_input": "2023-10-31T18:17:28.548233Z",
     "iopub.status.idle": "2023-10-31T18:17:33.220053Z",
     "shell.execute_reply.started": "2023-10-31T18:17:28.548197Z",
     "shell.execute_reply": "2023-10-31T18:17:33.219151Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:14:32.868362742Z",
     "start_time": "2023-10-31T20:14:32.818162906Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alsu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "576e5df59af45181"
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_text_pair(row, length_threshold=50):\n",
    "    reference = row['reference']\n",
    "    translation = row['translation']\n",
    "    length_diff = torch.tensor([row['lenght_diff']])  # Convert to tensor\n",
    "    similarity = torch.tensor([row['similarity']])  # Convert to tensor\n",
    "\n",
    "    # Tokenization\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    translation_tokens = word_tokenize(translation)\n",
    "\n",
    "    # Length normalization\n",
    "    pad_token = '<pad>'\n",
    "    padded_ref = reference_tokens[:length_threshold] + [pad_token] * max(0, length_threshold - len(reference_tokens))\n",
    "    padded_trans = translation_tokens[:length_threshold] + [pad_token] * max(0, length_threshold - len(translation_tokens))\n",
    "\n",
    "    return padded_ref, padded_trans, length_diff, similarity\n",
    "\n",
    "def preprocess_dataset(dataset, length_threshold=50):\n",
    "    preprocessed_data = pd.DataFrame()\n",
    "\n",
    "    (\n",
    "        preprocessed_data['reference'],\n",
    "        preprocessed_data['translation'],\n",
    "        preprocessed_data['lenght_diff'],\n",
    "        preprocessed_data['similarity']\n",
    "    ) = zip(*dataset.apply(lambda row: preprocess_text_pair(row, length_threshold), axis=1))\n",
    "\n",
    "    return preprocessed_data"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:17:35.929082Z",
     "iopub.execute_input": "2023-10-31T18:17:35.929690Z",
     "iopub.status.idle": "2023-10-31T18:17:35.938644Z",
     "shell.execute_reply.started": "2023-10-31T18:17:35.929654Z",
     "shell.execute_reply": "2023-10-31T18:17:35.937429Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:14:34.798032970Z",
     "start_time": "2023-10-31T20:14:34.796759484Z"
    }
   },
   "execution_count": 15,
   "outputs": [],
   "id": "e6046185ba1a8081"
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom Dataset Class\n",
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, data, test=False):\n",
    "        self.data = data\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        reference = self.data.iloc[idx]['reference']\n",
    "        translation = self.data.iloc[idx]['translation']\n",
    "        length_diff = self.data.iloc[idx]['lenght_diff']\n",
    "        similarity = self.data.iloc[idx]['similarity']\n",
    "    \n",
    "        if self.test:\n",
    "                    # During testing, return only what's needed for prediction\n",
    "            return {'reference': reference, 'translation': translation}\n",
    "        else:\n",
    "            # During training, return additional features\n",
    "            return {'reference': reference, 'translation': translation, 'lenght_diff': length_diff, 'similarity': similarity}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:17:38.210668Z",
     "iopub.execute_input": "2023-10-31T18:17:38.211269Z",
     "iopub.status.idle": "2023-10-31T18:17:38.218589Z",
     "shell.execute_reply.started": "2023-10-31T18:17:38.211230Z",
     "shell.execute_reply": "2023-10-31T18:17:38.217548Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:14:36.950970484Z",
     "start_time": "2023-10-31T20:14:36.946652257Z"
    }
   },
   "execution_count": 16,
   "outputs": [],
   "id": "e0bb6f1f54217172"
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare Data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "file_path = \"../data/raw/filtered.tsv\"\n",
    "raw_df = pd.read_csv(file_path, delimiter=\"\\t\")\n",
    "preprocessed_df = preprocess_dataset(raw_df)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:18:05.293378Z",
     "iopub.execute_input": "2023-10-31T18:18:05.293965Z",
     "iopub.status.idle": "2023-10-31T18:22:21.517612Z",
     "shell.execute_reply.started": "2023-10-31T18:18:05.293933Z",
     "shell.execute_reply": "2023-10-31T18:22:21.516782Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:16:34.782727276Z",
     "start_time": "2023-10-31T20:14:40.054720994Z"
    }
   },
   "execution_count": 17,
   "outputs": [],
   "id": "8f3d8f0053dc5293"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "preprocessed_df.to_csv('../data/interim/intermediate_dataset_1.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:18:58.520401800Z",
     "start_time": "2023-10-31T20:16:39.644853511Z"
    }
   },
   "id": "56e320a394d73ac7"
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the dataset into train and validation\n",
    "train_df, val_df = train_test_split(preprocessed_df, test_size=0.2, random_state=42)\n",
    "train_df_mini = train_df[:1000]\n",
    "val_df_mini = val_df[:500]\n",
    "train_dataset = ParaphraseDataset(train_df_mini)\n",
    "val_dataset = ParaphraseDataset(val_df_mini, True)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:43:08.685826Z",
     "iopub.execute_input": "2023-10-31T18:43:08.686705Z",
     "iopub.status.idle": "2023-10-31T18:43:08.929918Z",
     "shell.execute_reply.started": "2023-10-31T18:43:08.686668Z",
     "shell.execute_reply": "2023-10-31T18:43:08.928838Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:20:55.688825635Z",
     "start_time": "2023-10-31T20:20:55.499436886Z"
    }
   },
   "execution_count": 19,
   "outputs": [],
   "id": "c2fc6a152b528abf"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for _, row in preprocessed_df.iterrows():\n",
    "    reference_tokens = row['reference']\n",
    "    translation_tokens = row['translation']\n",
    "    \n",
    "    all_words.extend(reference_tokens)\n",
    "    all_words.extend(translation_tokens)\n",
    "\n",
    "vocab = {word: idx for idx, word in enumerate(set(all_words))}\n",
    "vocab_size = len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:21:19.519529887Z",
     "start_time": "2023-10-31T20:20:57.600880268Z"
    }
   },
   "id": "23a9b1c86ca3a061"
  },
  {
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100  \n",
    "hidden_dim = 128 \n",
    "length_threshold = 50"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:28:13.664154Z",
     "iopub.execute_input": "2023-10-31T18:28:13.664985Z",
     "iopub.status.idle": "2023-10-31T18:28:13.669318Z",
     "shell.execute_reply.started": "2023-10-31T18:28:13.664952Z",
     "shell.execute_reply": "2023-10-31T18:28:13.668394Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:21:21.307272826Z",
     "start_time": "2023-10-31T20:21:21.302653050Z"
    }
   },
   "execution_count": 21,
   "outputs": [],
   "id": "71d02586d3d4e08"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleParaphraseModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, sequence_length):\n",
    "        super(SimpleParaphraseModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, reference, length_diff, similarity):\n",
    "        embeds = self.embeddings(reference)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:28:15.217349Z",
     "iopub.execute_input": "2023-10-31T18:28:15.217708Z",
     "iopub.status.idle": "2023-10-31T18:28:15.226385Z",
     "shell.execute_reply.started": "2023-10-31T18:28:15.217679Z",
     "shell.execute_reply": "2023-10-31T18:28:15.225238Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:21:22.867790880Z",
     "start_time": "2023-10-31T20:21:22.865241627Z"
    }
   },
   "execution_count": 22,
   "outputs": [],
   "id": "fc32100e65c3bf2"
  },
  {
   "cell_type": "code",
   "source": [
    "# Instantiate the paraphrase model\n",
    "simple_paraphrase_model = SimpleParaphraseModel(vocab_size, embedding_dim, hidden_dim, 50).to('cuda')\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(simple_paraphrase_model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:28:17.406764Z",
     "iopub.execute_input": "2023-10-31T18:28:17.407361Z",
     "iopub.status.idle": "2023-10-31T18:28:17.736634Z",
     "shell.execute_reply.started": "2023-10-31T18:28:17.407328Z",
     "shell.execute_reply": "2023-10-31T18:28:17.735620Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:21:27.817117912Z",
     "start_time": "2023-10-31T20:21:25.278037406Z"
    }
   },
   "execution_count": 23,
   "outputs": [],
   "id": "2989234b9425b04"
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    references = [sample['reference'] for sample in batch]\n",
    "    translations = [sample['translation'] for sample in batch]\n",
    "    length_diffs = [sample['lenght_diff'] for sample in batch]\n",
    "    similarities = [sample['similarity'] for sample in batch]\n",
    "\n",
    "    # Flatten the list of tokens and convert them to tensors\n",
    "    all_reference_tokens = [token for reference_tokens in references for token in reference_tokens]\n",
    "    all_translation_tokens = [token for translation_tokens in translations for token in translation_tokens]\n",
    "\n",
    "    # Build the vocabulary from the tokens\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    vocab.update({word: idx + len(vocab) for idx, word in enumerate(set(all_reference_tokens + all_translation_tokens))})\n",
    "\n",
    "    # Convert tokens to indices and pad sequences to length 50\n",
    "    references_padded = pad_sequence(\n",
    "        [torch.tensor([vocab.get(token, vocab['<unk>']) for token in reference_tokens][:50]) for reference_tokens in references],\n",
    "        batch_first=True,\n",
    "        padding_value=vocab['<pad>']\n",
    "    )\n",
    "\n",
    "    translations_padded = pad_sequence(\n",
    "        [torch.tensor([vocab.get(token, vocab['<unk>']) for token in translation_tokens][:50]) for translation_tokens in translations],\n",
    "        batch_first=True,\n",
    "        padding_value=vocab['<pad>']\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'reference': references_padded,\n",
    "        'translation': translations_padded,\n",
    "        'lenght_diff': torch.tensor(length_diffs),\n",
    "        'similarity': torch.tensor(similarities)\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:28:19.652460Z",
     "iopub.execute_input": "2023-10-31T18:28:19.653209Z",
     "iopub.status.idle": "2023-10-31T18:28:19.662668Z",
     "shell.execute_reply.started": "2023-10-31T18:28:19.653176Z",
     "shell.execute_reply": "2023-10-31T18:28:19.661755Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:21:31.788240101Z",
     "start_time": "2023-10-31T20:21:31.785300082Z"
    }
   },
   "execution_count": 24,
   "outputs": [],
   "id": "fd9ef861f7234429"
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn_test(batch):\n",
    "    references = [sample['reference'] for sample in batch]\n",
    "    translations = [sample['translation'] for sample in batch]\n",
    "\n",
    "    # Flatten the list of tokens and convert them to tensors\n",
    "    all_reference_tokens = [token for reference_tokens in references for token in reference_tokens]\n",
    "    all_translation_tokens = [token for translation_tokens in translations for token in translation_tokens]\n",
    "\n",
    "    # Build the vocabulary from the tokens\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    vocab.update({word: idx + len(vocab) for idx, word in enumerate(set(all_reference_tokens + all_translation_tokens))})\n",
    "\n",
    "    # Convert tokens to indices and pad sequences to length 50\n",
    "    references_padded = pad_sequence(\n",
    "        [torch.tensor([vocab.get(token, vocab['<unk>']) for token in reference_tokens][:50]) for reference_tokens in references],\n",
    "        batch_first=True,\n",
    "        padding_value=vocab['<pad>']\n",
    "    )\n",
    "\n",
    "    translations_padded = pad_sequence(\n",
    "        [torch.tensor([vocab.get(token, vocab['<unk>']) for token in translation_tokens][:50]) for translation_tokens in translations],\n",
    "        batch_first=True,\n",
    "        padding_value=vocab['<pad>']\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'reference': references_padded,\n",
    "        'translation': translations_padded\n",
    "    }"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-31T18:40:41.934991Z",
     "iopub.execute_input": "2023-10-31T18:40:41.935376Z",
     "iopub.status.idle": "2023-10-31T18:40:41.945351Z",
     "shell.execute_reply.started": "2023-10-31T18:40:41.935343Z",
     "shell.execute_reply": "2023-10-31T18:40:41.944137Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:21:34.385159531Z",
     "start_time": "2023-10-31T20:21:34.344764650Z"
    }
   },
   "execution_count": 25,
   "outputs": [],
   "id": "a0cf838d1150cab1"
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataloader for training and test\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn_test)"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:43:14.446597Z",
     "iopub.execute_input": "2023-10-31T18:43:14.446971Z",
     "iopub.status.idle": "2023-10-31T18:43:14.452401Z",
     "shell.execute_reply.started": "2023-10-31T18:43:14.446943Z",
     "shell.execute_reply": "2023-10-31T18:43:14.451387Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:21:36.520843979Z",
     "start_time": "2023-10-31T20:21:36.518026949Z"
    }
   },
   "execution_count": 26,
   "outputs": [],
   "id": "8488d6bc0d222e9f"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_perplexity = 0  # Track cumulative perplexity\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        reference = batch['reference'].to('cuda')\n",
    "        translation = batch['translation'].to('cuda')\n",
    "        length_diff = batch['lenght_diff'].to('cuda')\n",
    "        similarity = batch['similarity'].to('cuda')\n",
    "        \n",
    "        # Model forward pass\n",
    "        output = simple_paraphrase_model(reference, length_diff, similarity)\n",
    "        loss = criterion(output.view(-1, vocab_size), translation.view(-1))\n",
    "\n",
    "        # Compute perplexity\n",
    "        perplexity = torch.exp(loss)  # Using exponential to get perplexity\n",
    "        total_loss += loss.item()\n",
    "        total_perplexity += perplexity.item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    average_perplexity = total_perplexity / len(train_dataloader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}, Perplexity: {average_perplexity:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:33:59.254848Z",
     "iopub.execute_input": "2023-10-31T18:33:59.255214Z",
     "iopub.status.idle": "2023-10-31T18:34:27.751353Z",
     "shell.execute_reply.started": "2023-10-31T18:33:59.255184Z",
     "shell.execute_reply": "2023-10-31T18:34:27.750399Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:21:39.640670588Z",
     "start_time": "2023-10-31T20:21:38.618074698Z"
    }
   },
   "execution_count": 27,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.79 GiB (GPU 0; 5.80 GiB total capacity; 5.51 GiB already allocated; 135.44 MiB free; 5.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m# Backward and optimize\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 27\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     30\u001B[0m average_loss \u001B[38;5;241m=\u001B[39m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataloader)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1.79 GiB (GPU 0; 5.80 GiB total capacity; 5.51 GiB already allocated; 135.44 MiB free; 5.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "id": "b35eb347af3c49f2"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Testing loop\n",
    "simple_paraphrase_model.eval()  # Set the model to evaluation mode\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation during testing\n",
    "    for batch in test_dataloader:  # Assuming you have a DataLoader for testing data\n",
    "        reference = batch['reference'].to('cuda')\n",
    "        translation = batch['translation'].to('cuda')\n",
    "        \n",
    "        # Model forward pass\n",
    "        output = simple_paraphrase_model(reference, length_diff, similarity)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output.view(-1, vocab_size), translation.view(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "average_loss = total_loss / len(test_dataloader)\n",
    "print(f'Test Loss: {average_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-10-31T18:43:17.384058Z",
     "iopub.execute_input": "2023-10-31T18:43:17.384789Z",
     "iopub.status.idle": "2023-10-31T18:43:18.061201Z",
     "shell.execute_reply.started": "2023-10-31T18:43:17.384755Z",
     "shell.execute_reply": "2023-10-31T18:43:18.060230Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-10-31T19:52:09.521513899Z"
    }
   },
   "execution_count": null,
   "outputs": [],
   "id": "db96ddbf4aa6fa13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint = {'model': SimpleParaphraseModel,\n",
    "              'state_dict': simple_paraphrase_model.state_dict(),\n",
    "              'optimizer' : optimizer.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c325936dc1f38d3"
  }
 ]
}
